{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:13.213106Z",
     "start_time": "2025-02-03T05:25:13.206226Z"
    }
   },
   "source": [
    "#### PARAMS ####\n",
    "NUM_ROUNDS = 100\n",
    "NUM_CLIENTS = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_SHADOW_MODELS = 5\n",
    "GLOBAL_MODEL_EPOCHS = 10\n",
    "SHADOW_TRAIN_ROUNDS = 10\n",
    "SHADOW_DATA_FRACTION = 0.01\n",
    "\n",
    "PROB_RANGE = 0.1\n",
    "\n",
    "ENTRENO_ANTES = True  # True: Shadow models entrenan antes, False: Shadow models infieren dinámicamente\n",
    "\n",
    "# Global constants\n",
    "FRACTION = 0.2  # Fraction of data to be used for training\n",
    "\n",
    "APLICAR_RUIDO = False  # Determina si se aplica ruido en el proceso.\n",
    "RUIDO_ANTES = False # Si True, el ruido se aplica antes del entrenamiento; si False, después.\n",
    "RUIDO_PER = 0.4  # Proporción de datos afectados por ruido.\n",
    "NOISE_STD = 0.2  # Desviación estándar del ruido aplicado.\n",
    "EPSILON = 1.0  # Parámetro de privacidad (menor = más privacidad, más ruido)\n",
    "DELTA = 1e-5  # Delta para ruido gaussiano (puedes dejarlo en None si solo usas Laplace)\n",
    "SENSITIVITY = 1.0  # Sensibilidad del ruido (depende del rango de los datos o actualizaciones)\n",
    "\n",
    "\n",
    "LABEL_FLIPPING = False\n",
    "FLIPPING_ANTES = False\n",
    "PROB_FLIP_0 = 0.2\n",
    "PROB_FLIP_1 = 0.2\n",
    "FLIP_TARGET = \"Slice\"\n",
    "\n",
    "PROPERTY_THRESHOLD = 0.5\n",
    "LEARNING_RATE = 0.1\n",
    "DATA_FILE_PATH = 'label_bi_10.csv'\n",
    "\n",
    "PREFIJO_SAVE = \"resultados_iniciales\"\n",
    "RESULTS_CSV_FILE = f'{PREFIJO_SAVE}/federated_learning_results.csv'\n",
    "SHADOW_WEIGHTS_CSV_FILE = f\"{PREFIJO_SAVE}/weights.csv\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:26.923545Z",
     "start_time": "2025-02-03T05:25:13.214112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tqdm\n",
    "from numpy.random import laplace, normal\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n"
   ],
   "id": "3bb448f0fe1ab29f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:26.928056Z",
     "start_time": "2025-02-03T05:25:26.924551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### LOG ####\n",
    "LOG_DIR = 'logs'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "if not os.path.exists(PREFIJO_SAVE):\n",
    "    os.makedirs(PREFIJO_SAVE)\n",
    "    \n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(LOG_DIR, 'execution.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "28c8288548d1c03e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:26.934765Z",
     "start_time": "2025-02-03T05:25:26.929061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_configuration():\n",
    "    \"\"\"\n",
    "    Valida las configuraciones de parámetros y asegura que las combinaciones sean coherentes.\n",
    "    \"\"\"\n",
    "    logger.info(\"Validating configuration...\")\n",
    "\n",
    "    # Verificar configuraciones inválidas\n",
    "    if not ENTRENO_ANTES and FLIPPING_ANTES:\n",
    "        raise ValueError(\"FLIPPING_ANTES=True no tiene sentido cuando ENTRENO_ANTES=False.\")\n",
    "\n",
    "    # Informar configuraciones redundantes\n",
    "    if not APLICAR_RUIDO and RUIDO_ANTES:\n",
    "        raise ValueError(\"RUIDO_ANTES=True no tiene sentido cuando APLICAR_RUIDO=False.\")\n",
    "    if not LABEL_FLIPPING and FLIPPING_ANTES:\n",
    "        raise ValueError(\"FLIPPING_ANTES=True no tiene sentido cuando LABEL_FLIPPING=False.\")\n",
    "\n",
    "    if not APLICAR_RUIDO and not LABEL_FLIPPING:\n",
    "        logger.info(\"Ni ruido ni label flipping están activados. El experimento no incluye perturbaciones en los datos.\")\n",
    "\n",
    "    # Validar parámetros relacionados con ruido\n",
    "    if APLICAR_RUIDO:\n",
    "        if EPSILON <= 0:\n",
    "            raise ValueError(\"EPSILON debe ser mayor que 0.\")\n",
    "        if DELTA is not None and not (0 < DELTA < 1):\n",
    "            raise ValueError(\"DELTA debe estar entre 0 y 1 si se usa ruido gaussiano.\")\n",
    "        if SENSITIVITY <= 0:\n",
    "            raise ValueError(\"SENSITIVITY debe ser mayor que 0.\")\n",
    "\n",
    "\n",
    "    # Validar parámetros relacionados con flipping\n",
    "    if LABEL_FLIPPING:\n",
    "        if not (0 <= PROB_FLIP_0 <= 1):\n",
    "            raise ValueError(\"PROB_FLIP_0 debe estar entre 0 y 1.\")\n",
    "        if not (0 <= PROB_FLIP_1 <= 1):\n",
    "            raise ValueError(\"PROB_FLIP_1 debe estar entre 0 y 1.\")\n",
    "\n",
    "    logger.info(\"Configuration validation completed successfully.\")\n"
   ],
   "id": "10cc2fca338cd9cf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:26.943889Z",
     "start_time": "2025-02-03T05:25:26.935770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### DATA ####\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "\n",
    "    logger.info(f\"Loading and preprocessing data from {file_path}\")\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Select necessary columns for the experiment\n",
    "    necessary_columns = [\n",
    "        'Src IP', 'Src Port', 'Dst Port', 'Protocol', 'Flow Duration', 'Total Fwd Packet',\n",
    "        'Fwd Packet Length Std', 'ACK Flag Count', 'Fwd Seg Size Min', 'label', 'Slice',\n",
    "    ]\n",
    "    df = df[necessary_columns]\n",
    "\n",
    "    # Randomly sample the data based on the FRACTION constant and drop missing values\n",
    "    df = df.sample(frac=FRACTION).reset_index(drop=True)\n",
    "    df = df.dropna()\n",
    "    print(f\"Tamaño del dataset {df.shape}\")\n",
    "    \n",
    "    # Separate features (X) from the labels (y_label) and the property indicator (y_slice)\n",
    "    X = df.drop(['label', 'Slice'], axis=1)\n",
    "    y_label = df['label']\n",
    "    y_slice = df['Slice']\n",
    "    \n",
    "    logger.info(f\"Columns after loading: {list(df.columns)}\")\n",
    "\n",
    "    # Ensure that the dataset contains enough data for further processing\n",
    "    assert len(X) > 0, \"The dataset does not contain enough data after preprocessing.\"\n",
    "\n",
    "    return X, y_label, y_slice\n",
    "\n",
    "def create_client_data(X, y_label, y_slice, num_clients=NUM_CLIENTS):\n",
    "    \"\"\"\n",
    "    Divide los datos para los clientes y aplica ruido o flipping si está configurado.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating data for {num_clients} clients with security configurations\")\n",
    "\n",
    "    # Mezclar y dividir datos\n",
    "    data = pd.concat([X, y_label, y_slice], axis=1).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    X, y_label, y_slice = data.iloc[:, :-2], data.iloc[:, -2], data.iloc[:, -1]\n",
    "\n",
    "    client_data = []\n",
    "\n",
    "    # Separar datos por propiedad\n",
    "    X_with_property = X[y_slice == 1]\n",
    "    y_label_with_property = y_label[y_slice == 1]\n",
    "    X_without_property = X[y_slice == 0]\n",
    "    y_label_without_property = y_label[y_slice == 0]\n",
    "\n",
    "    min_data_size = min(len(X_with_property) // (num_clients // 2), len(X_without_property) // (num_clients // 2))\n",
    "\n",
    "    for i in range(num_clients // 2):\n",
    "        client_data.append({\n",
    "            'X': X_with_property.iloc[i * min_data_size:(i + 1) * min_data_size],\n",
    "            'y_label': y_label_with_property.iloc[i * min_data_size:(i + 1) * min_data_size].copy(),\n",
    "            'y_slice': 1,\n",
    "            'has_property': True\n",
    "        })\n",
    "\n",
    "        client_data.append({\n",
    "            'X': X_without_property.iloc[i * min_data_size:(i + 1) * min_data_size],\n",
    "            'y_label': y_label_without_property.iloc[i * min_data_size:(i + 1) * min_data_size].copy(),\n",
    "            'y_slice': 0,\n",
    "            'has_property': False\n",
    "        })\n",
    "\n",
    "    # Aplicar ruido y flipping antes del entrenamiento (si está configurado)\n",
    "    if FLIPPING_ANTES:\n",
    "        logger.info(\"Applying label flipping BEFORE local training\")\n",
    "        for client in client_data:\n",
    "            client['y_label'] = apply_flipping(client['y_label'], FLIP_TARGET, PROB_FLIP_0, PROB_FLIP_1)\n",
    "\n",
    "    if RUIDO_ANTES:\n",
    "        logger.info(\"Applying noise BEFORE local training\")\n",
    "        for client in client_data:\n",
    "            client['X'] = apply_noise(client['X'], RUIDO_PER, NOISE_STD, EPSILON, DELTA, SENSITIVITY, ruido_antes=True)\n",
    "\n",
    "    return client_data\n"
   ],
   "id": "f275bdad6a1c8604",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:26.964371Z",
     "start_time": "2025-02-03T05:25:26.944894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### MODELS ####\n",
    "def split_data_for_models(X, y_label, y_slice, shadow_fraction=SHADOW_DATA_FRACTION):\n",
    "    \"\"\"\n",
    "    Divide los datos en conjuntos para el modelo global y los modelos sombra.\n",
    "    \"\"\"\n",
    "    logger.info(\"Dividiendo datos para el modelo global y los modelos sombra.\")\n",
    "\n",
    "    num_shadow_samples = int(len(X) * shadow_fraction)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    shadow_indices = indices[:num_shadow_samples]\n",
    "    global_indices = indices[num_shadow_samples:]\n",
    "\n",
    "    X_shadow = X.iloc[shadow_indices]\n",
    "    y_label_shadow = y_label.iloc[shadow_indices]\n",
    "    y_slice_shadow = y_slice.iloc[shadow_indices]\n",
    "\n",
    "    X_global = X.iloc[global_indices]\n",
    "    y_label_global = y_label.iloc[global_indices]\n",
    "    y_slice_global = y_slice.iloc[global_indices]\n",
    "\n",
    "    logger.info(f\"Datos divididos: {len(X_global)} para el modelo global, {len(X_shadow)} para los modelos sombra.\")\n",
    "    return (X_global, y_label_global, y_slice_global), (X_shadow, y_label_shadow, y_slice_shadow), len(X_shadow)\n",
    "\n",
    "def create_global_model(input_shape):\n",
    "    logger.info(f\"Creating global model with input shape {input_shape}\")\n",
    "    \n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_shape,)),  # Input layer with the specified input shape\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 units and ReLU activation\n",
    "        tf.keras.layers.Dense(32, activation='relu'),  # Hidden layer with 32 units and ReLU activation\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with Adam optimizer and binary crossentropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_shadow_model(input_shape):\n",
    "\n",
    "    logger.info(f\"Creating shadow model with input shape {input_shape}\")\n",
    "    return create_global_model(input_shape)\n",
    "\n",
    "def initialize_clients(client_data, global_model, global_model_epochs, batch_size):\n",
    "    logger.info(\"Initializing simulated clients in parallel\")\n",
    "\n",
    "    def init_client(client_id, data):\n",
    "        # Pasar el modelo global completo\n",
    "        return SimulatedFlowerClient(client_id, data, global_model, global_model_epochs, batch_size)\n",
    "\n",
    "    # Paralelizar la inicialización de los clientes\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(init_client, i, data) for i, data in enumerate(client_data)]\n",
    "        clients = [f.result() for f in futures]\n",
    "\n",
    "    return clients\n",
    "\n",
    "class SimulatedFlowerClient:\n",
    "    def __init__(self, cid, data, global_model, global_model_epochs, batch_size):\n",
    "        \"\"\"\n",
    "        Inicializa un cliente simulado con sus datos, modelo y parámetros.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Initializing simulated client {cid}\")\n",
    "        self.cid = cid\n",
    "        self.data = data\n",
    "        self.global_model_epochs = global_model_epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Usar el modelo global proporcionado y compilarlo\n",
    "        self.model = tf.keras.models.clone_model(global_model)\n",
    "        self.model.set_weights(global_model.get_weights())\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Devuelve los parámetros actuales del modelo del cliente.\"\"\"\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters):\n",
    "        \"\"\"\n",
    "        Entrena el modelo del cliente con los parámetros globales y aplica ruido/flipping si es necesario.\n",
    "        \"\"\"\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.data['X'], self.data['y_label'], epochs=self.global_model_epochs, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "        # Calcular actualizaciones de los pesos\n",
    "        updates = [new_w - old_w for new_w, old_w in zip(self.model.get_weights(), parameters)]\n",
    "\n",
    "        # Aplicar ruido a las actualizaciones si está configurado\n",
    "        if APLICAR_RUIDO and not RUIDO_ANTES:\n",
    "            logger.info(f\"Applying noise to updates for client {self.cid} AFTER training\")\n",
    "            updates = apply_noise(updates, RUIDO_PER, NOISE_STD, EPSILON, DELTA, SENSITIVITY, ruido_antes=False)\n",
    "\n",
    "        # Aplicar flipping después del entrenamiento si está configurado\n",
    "        if LABEL_FLIPPING and not FLIPPING_ANTES:\n",
    "            logger.info(f\"Applying label flipping AFTER local training for client {self.cid}\")\n",
    "            self.data['y_label'] = apply_flipping(self.data['y_label'], FLIP_TARGET, PROB_FLIP_0, PROB_FLIP_1)\n",
    "\n",
    "        return self.model.get_weights(), len(self.data['X']), {\"updates\": updates}\n",
    "\n",
    "    def evaluate(self, parameters):\n",
    "        \"\"\"Evalúa el modelo global en los datos locales del cliente.\"\"\"\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(self.data['X'], self.data['y_label'], verbose=0)\n",
    "        return loss, len(self.data['X']), {\"accuracy\": accuracy}\n",
    "\n",
    "def train_shadow_models(X_shadow, y_label_shadow, y_slice_shadow, num_shadow_models, global_model, global_model_epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Entrena los shadow models usando las actualizaciones de los pesos.\n",
    "    \"\"\"\n",
    "    if not ENTRENO_ANTES:\n",
    "        logger.info(\"Skipping shadow model training because ENTRENO_ANTES is False.\")\n",
    "        return []\n",
    "    \n",
    "    logger.info(f\"Training {num_shadow_models} shadow models prior to federated learning.\")\n",
    "    \n",
    "    # Resetear el índice de y_slice_shadow para asegurar acceso por índice entero\n",
    "    y_slice_shadow = y_slice_shadow.reset_index(drop=True)\n",
    "    \n",
    "    # Simular actualizaciones para los datos de shadow\n",
    "    X_shadow_data = []\n",
    "    y_shadow_labels = []\n",
    "    \n",
    "    logging_count = 0\n",
    "    for idx in range(len(X_shadow)):\n",
    "        \n",
    "        if logging_count % 10 == 0:\n",
    "            logger.info(f'Processing shadow model {idx + 1}/{len(X_shadow)}')\n",
    "        logging_count += 1\n",
    "        \n",
    "        shadow_model = tf.keras.models.clone_model(global_model)\n",
    "        shadow_model.set_weights(global_model.get_weights())\n",
    "        shadow_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        initial_weights = shadow_model.get_weights()\n",
    "        \n",
    "        # Entrenar el modelo en un único ejemplo para generar actualizaciones\n",
    "        shadow_model.fit(X_shadow[idx:idx+1], y_label_shadow[idx:idx+1],\n",
    "                         epochs=1, batch_size=1, verbose=0)\n",
    "        final_weights = shadow_model.get_weights()\n",
    "        \n",
    "        # Calcular las actualizaciones de los pesos\n",
    "        updates = [final_w - init_w for final_w, init_w in zip(final_weights, initial_weights)]\n",
    "        flattened_updates = np.concatenate([u.flatten() for u in updates])\n",
    "\n",
    "        X_shadow_data.append(flattened_updates)\n",
    "        y_shadow_labels.append(y_slice_shadow[idx])  # Ahora se puede acceder con índice entero\n",
    "    \n",
    "    # Convertir listas a numpy arrays\n",
    "    X_shadow_data = np.array(X_shadow_data)\n",
    "    y_shadow_labels = np.array(y_shadow_labels)\n",
    "    \n",
    "    # Entrenar los modelos sombra con las actualizaciones generadas\n",
    "    shadow_models = []\n",
    "    for i in range(num_shadow_models):\n",
    "        logger.info(f\"Training shadow model {i + 1}/{num_shadow_models}\")\n",
    "        shadow_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(X_shadow_data.shape[1],)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        shadow_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        shadow_model.fit(X_shadow_data, y_shadow_labels,\n",
    "                         epochs=global_model_epochs, batch_size=batch_size, verbose=0)\n",
    "        shadow_models.append(shadow_model)\n",
    "    \n",
    "    logger.info(\"All shadow models trained successfully.\")\n",
    "    return shadow_models\n",
    "\n",
    "def dynamic_inference_with_custom_model(averaged_updates):\n",
    "    \"\"\"\n",
    "    Realiza inferencia dinámica con un modelo sombra diseñado para ENTRENO_ANTES=False.\n",
    "    \"\"\"\n",
    "    input_dim = 2753  # Ajustar según las dimensiones esperadas\n",
    "    logger.info(f\"Dynamic inference with input dimension {input_dim}\")\n",
    "\n",
    "    # Ajustar las dimensiones del input\n",
    "    if len(averaged_updates) > input_dim:\n",
    "        logger.warning(f\"Truncating updates from {len(averaged_updates)} to {input_dim}\")\n",
    "        adjusted_updates = averaged_updates[:input_dim]\n",
    "    elif len(averaged_updates) < input_dim:\n",
    "        logger.warning(f\"Padding updates from {len(averaged_updates)} to {input_dim}\")\n",
    "        adjusted_updates = np.pad(averaged_updates, (0, input_dim - len(averaged_updates)), mode='constant')\n",
    "    else:\n",
    "        adjusted_updates = averaged_updates\n",
    "\n",
    "    # Normalizar las actualizaciones\n",
    "    normalized_updates = (adjusted_updates - np.mean(adjusted_updates)) / (np.std(adjusted_updates) + 1e-8)\n",
    "    flattened_updates = normalized_updates.reshape(1, -1)\n",
    "\n",
    "    # Realizar la inferencia\n",
    "    shadow_model = custom_model(input_dim)\n",
    "    prob = shadow_model.predict(flattened_updates, verbose=0)[0][0]\n",
    "    logger.info(f\"Dynamic inference probability: {prob:.4f}\")\n",
    "    return prob\n",
    "\n",
    "\n",
    "def infer_property(shadow_models, updates):\n",
    "    \"\"\"\n",
    "    Infieren la probabilidad de la propiedad a partir de las actualizaciones de los pesos.\n",
    "    \"\"\"\n",
    "    flattened_updates = np.concatenate([u.flatten() for u in updates]).reshape(1, -1)\n",
    "    probabilities = []\n",
    "    for model in shadow_models:\n",
    "        prob = model.predict(flattened_updates, verbose=0)[0][0]\n",
    "        probabilities.append(prob)\n",
    "\n",
    "    # Promediar las probabilidades predichas por los modelos sombra\n",
    "    return np.mean(probabilities)\n",
    "\n",
    "def average_client_updates(client_updates):\n",
    "    \"\"\"\n",
    "    Promedia las actualizaciones de los clientes asegurándose de que todas tengan la misma longitud.\n",
    "    \"\"\"\n",
    "    # Asegurar que todas las actualizaciones tengan la misma estructura\n",
    "    num_layers = len(client_updates[0])\n",
    "    averaged_updates = []\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_updates = [update[layer_idx] for update in client_updates]\n",
    "        averaged_layer = np.mean(layer_updates, axis=0)\n",
    "        averaged_updates.append(averaged_layer)\n",
    "    \n",
    "    return averaged_updates\n",
    "\n",
    "def custom_model(input_shape):\n",
    "    \"\"\"\n",
    "    Crea un modelo específico para inferencia en el modo ENTRENO_ANTES=False.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating custom shadow model with input shape {input_shape}\")\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_shape,)),  # Input layer\n",
    "\n",
    "        # Primera capa densa con normalización\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        # Segunda capa para aprender relaciones complejas\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Regularización para evitar sobreajuste\n",
    "\n",
    "        # Capa de salida para predicción binaria\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Predicción de probabilidad de \"Slice1\"\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ],
   "id": "bb63cc478e698133",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:26.974562Z",
     "start_time": "2025-02-03T05:25:26.965377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### SEGURIDAD ####\n",
    "def apply_label_flipping(y_data, flip_target, prob_flip_0, prob_flip_1):\n",
    "    \"\"\"\n",
    "    Aplica flipping de etiquetas o propiedades basado en el parámetro flip_target.\n",
    "    \"\"\"\n",
    "    if flip_target not in [\"label\", \"Slice\"]:\n",
    "        raise ValueError(f\"Invalid flip_target: {flip_target}. Must be 'label' or 'Slice'.\")\n",
    "\n",
    "    if isinstance(y_data, pd.Series):\n",
    "        # Caso Series: se asume que el target es directamente la serie\n",
    "        target_data = y_data\n",
    "\n",
    "    elif isinstance(y_data, pd.DataFrame):\n",
    "        # Caso DataFrame: se accede a la columna específica indicada por flip_target\n",
    "        if flip_target in y_data.columns:\n",
    "            target_data = y_data[flip_target]\n",
    "        else:\n",
    "            raise KeyError(f\"Column '{flip_target}' is not found in DataFrame data.\")\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported data type: {type(y_data)}. Must be pandas DataFrame or Series.\")\n",
    "\n",
    "    # Flipping 0 -> 1\n",
    "    flip_0_indices = target_data[target_data == 0].index\n",
    "    num_flips_0_to_1 = int(len(flip_0_indices) * prob_flip_0)\n",
    "    flipped_0_indices = np.random.choice(flip_0_indices, num_flips_0_to_1, replace=False)\n",
    "    target_data.loc[flipped_0_indices] = 1\n",
    "\n",
    "    # Flipping 1 -> 0\n",
    "    flip_1_indices = target_data[target_data == 1].index\n",
    "    num_flips_1_to_0 = int(len(flip_1_indices) * prob_flip_1)\n",
    "    flipped_1_indices = np.random.choice(flip_1_indices, num_flips_1_to_0, replace=False)\n",
    "    target_data.loc[flipped_1_indices] = 0\n",
    "\n",
    "    logger.info(f\"Flipped {num_flips_0_to_1} entries from 0 -> 1 and {num_flips_1_to_0} from 1 -> 0.\")\n",
    "\n",
    "    return target_data\n",
    "\n",
    "\n",
    "def add_continuous_noise(data, ruido_per, ruido_std, epsilon, delta=None, sensitivity=1.0, ruido_antes=True):\n",
    "    \"\"\"\n",
    "    Aplica ruido gaussiano o laplaciano según los principios de privacidad diferencial.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pd.DataFrame o list, datos o actualizaciones a las que se aplicará ruido.\n",
    "    - ruido_per: float, proporción de muestras afectadas (0 a 1).\n",
    "    - ruido_std: float, desviación estándar del ruido.\n",
    "    - epsilon: float, parámetro de privacidad diferencial (mayor = menos ruido).\n",
    "    - delta: float (opcional), requerido solo para ruido gaussiano.\n",
    "    - sensitivity: float, sensibilidad del ruido (magnitud máxima de cambio).\n",
    "    - ruido_antes: bool, indica si el ruido se aplica antes o después del entrenamiento (solo para logs).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame o lista con ruido aplicado.\n",
    "    \"\"\"\n",
    "    if not (0 <= ruido_per <= 1):\n",
    "        raise ValueError(\"El parámetro ruido_per debe estar entre 0 y 1.\")\n",
    "\n",
    "    if epsilon <= 0:\n",
    "        raise ValueError(\"El parámetro epsilon debe ser mayor que 0.\")\n",
    "\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # Identificar columnas continuas si se aplica a datos\n",
    "        continuous_columns = data.select_dtypes(include=['float64', 'int64']).columns.difference(['label', 'Slice'])\n",
    "        logger.info(f\"Columnas continuas seleccionadas para aplicar ruido: {list(continuous_columns)}\")\n",
    "\n",
    "        # Seleccionar filas que recibirán ruido\n",
    "        num_samples = int(len(data) * ruido_per)\n",
    "        noisy_indices = np.random.choice(data.index, size=num_samples, replace=False)\n",
    "        logger.info(f\"Número de muestras seleccionadas para ruido: {num_samples} de {len(data)}\")\n",
    "\n",
    "        # Escoger distribución de ruido\n",
    "        if epsilon > 0:\n",
    "            if delta is not None:\n",
    "                # Ruido Gaussiano\n",
    "                noise_scale = sensitivity * np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n",
    "                logger.info(f\"Aplicando ruido Gaussiano con privacidad diferencial {'antes' if ruido_antes else 'después'} del entrenamiento.\")\n",
    "                noise = normal(0, noise_scale, size=(num_samples, len(continuous_columns)))\n",
    "            else:\n",
    "                # Ruido Laplaciano\n",
    "                noise_scale = sensitivity / epsilon\n",
    "                logger.info(f\"Aplicando ruido Laplaciano con privacidad diferencial {'antes' if ruido_antes else 'después'} del entrenamiento.\")\n",
    "                noise = laplace(0, noise_scale, size=(num_samples, len(continuous_columns)))\n",
    "        else:\n",
    "            # Usar ruido estándar directo (NOISE_STD)\n",
    "            logger.warning(f\"Usando ruido estándar directo sin privacidad diferencial {'antes' if ruido_antes else 'después'} del entrenamiento.\")\n",
    "            noise = normal(0, ruido_std, size=(num_samples, len(continuous_columns)))\n",
    "\n",
    "        # Aplicar ruido a las columnas seleccionadas\n",
    "        noisy_data = data.copy()\n",
    "        noisy_data.loc[noisy_indices, continuous_columns] += noise\n",
    "        return noisy_data\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        # Asumir que es una lista de actualizaciones de los pesos\n",
    "        logger.info(f\"Aplicando ruido a actualizaciones de los pesos {'antes' if ruido_antes else 'después'} del entrenamiento\")\n",
    "        if epsilon > 0:\n",
    "            if delta is not None:\n",
    "                # Ruido Gaussiano\n",
    "                noise_scale = sensitivity * np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n",
    "                logger.info(f\"Aplicando ruido Gaussiano con privacidad diferencial a los pesos {'antes' if ruido_antes else 'después'} del entrenamiento.\")\n",
    "                noisy_updates = [u + normal(0, noise_scale, size=u.shape) for u in data]\n",
    "            else:\n",
    "                # Ruido Laplaciano\n",
    "                noise_scale = sensitivity / epsilon\n",
    "                logger.info(f\"Aplicando ruido Laplaciano con privacidad diferencial a los pesos {'antes' if ruido_antes else 'después'} del entrenamiento.\")\n",
    "                noisy_updates = [u + laplace(0, noise_scale, size=u.shape) for u in data]\n",
    "        else:\n",
    "            # Usar ruido estándar directo (NOISE_STD)\n",
    "            logger.warning(f\"Usando ruido estándar directo sin privacidad diferencial para los pesos {'antes' if ruido_antes else 'después'} del entrenamiento.\")\n",
    "            noisy_updates = [u + normal(0, ruido_std, size=u.shape) for u in data]\n",
    "        return noisy_updates\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"El parámetro 'data' debe ser un DataFrame o una lista.\")\n",
    "    \n",
    "def apply_noise(data, ruido_per, ruido_std, epsilon, delta, sensitivity, ruido_antes):\n",
    "    \"\"\"\n",
    "    Aplica ruido con privacidad diferencial a los datos o actualizaciones.\n",
    "    \"\"\"\n",
    "    return add_continuous_noise(data, ruido_per, ruido_std, epsilon, delta, sensitivity, ruido_antes)\n",
    "\n",
    "\n",
    "def apply_flipping(data, flip_target, prob_flip_0, prob_flip_1):\n",
    "    \"\"\"\n",
    "    Aplica flipping de etiquetas o propiedades basado en configuraciones.\n",
    "    \"\"\"\n",
    "    return apply_label_flipping(data, flip_target, prob_flip_0, prob_flip_1)\n"
   ],
   "id": "3d4f8900b3c0e38e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:26.993408Z",
     "start_time": "2025-02-03T05:25:26.975566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### OUTPUTS ####\n",
    "\n",
    "def calculate_correct_predictions(results, dynamic_threshold=0.50):\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "    threshold = dynamic_threshold\n",
    "\n",
    "    # Ignorar las primeras tres rondas\n",
    "    for i in range(3, len(results)):\n",
    "        current_prob = results[i]['property_probability']\n",
    "        previous_prob = results[i - 1]['property_probability']\n",
    "        previous_property = results[i - 1]['has_property']\n",
    "        current_property = results[i]['has_property']\n",
    "\n",
    "        # Ajustar el threshold dinámico\n",
    "        if current_prob > previous_prob:\n",
    "            threshold = max(threshold, (current_prob + previous_prob) / 2)\n",
    "        elif current_prob < previous_prob:\n",
    "            threshold = min(threshold, (current_prob + previous_prob) / 2)\n",
    "\n",
    "        # Clasificar movimientos según las reglas dadas\n",
    "        if not previous_property and not current_property:  # Ambas sin propiedad\n",
    "            if abs(current_prob - previous_prob) < threshold * 0.1:\n",
    "                correct_predictions += 1\n",
    "            else:\n",
    "                incorrect_predictions += 1\n",
    "        elif previous_property and not current_property:  # De propiedad a sin propiedad\n",
    "            if current_prob < previous_prob:\n",
    "                correct_predictions += 1\n",
    "            else:\n",
    "                incorrect_predictions += 1\n",
    "        elif not previous_property and current_property:  # De sin propiedad a propiedad\n",
    "            if current_prob > previous_prob:\n",
    "                correct_predictions += 1\n",
    "            else:\n",
    "                incorrect_predictions += 1\n",
    "        elif previous_property and current_property:  # Ambas con propiedad\n",
    "            if abs(current_prob - previous_prob) < threshold * 0.1:\n",
    "                correct_predictions += 1\n",
    "            else:\n",
    "                incorrect_predictions += 1\n",
    "\n",
    "    total_predictions = correct_predictions + incorrect_predictions\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "    logger.info(f\"Correct predictions: {correct_predictions}, Incorrect predictions: {incorrect_predictions}, Accuracy: {accuracy:.4f}\")\n",
    "    return correct_predictions, incorrect_predictions, threshold\n",
    "\n",
    "\n",
    "def plot_combined_probability_loss_accuracy(results, output_path):\n",
    "    \"\"\"\n",
    "    Genera una imagen con dos gráficos:\n",
    "    - Superior: Evolución de la probabilidad por ronda, ignorando las 3 primeras rondas.\n",
    "    - Inferior: Evolución de la pérdida y precisión por ronda.\n",
    "    \"\"\"\n",
    "    round_nums = [r['round'] for r in results][3:]\n",
    "    probabilities = [r['property_probability'] for r in results][3:]\n",
    "    losses = [r['loss'] for r in results][3:]\n",
    "    accuracies = [r['accuracy'] for r in results][3:]\n",
    "    property_present = [r['has_property'] for r in results][3:]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Gráfico 1: Probabilidad por ronda\n",
    "    axs[0].plot(round_nums, probabilities, marker='o', color='blue', label='Property Probability')\n",
    "    for round_, prob, present in zip(round_nums, probabilities, property_present):\n",
    "        if present:\n",
    "            axs[0].scatter(round_, prob, color='red', zorder=5, label='Rounds with Property' if round_nums.index(round_) == 0 else \"\")\n",
    "    axs[0].set_xlabel('Round')\n",
    "    axs[0].set_ylabel('Property Probability')\n",
    "    axs[0].set_title('Property Probability by Round (ignoring first 3 rounds)')\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Gráfico 2: Pérdida y Precisión\n",
    "    ax1 = axs[1]\n",
    "    ax1.plot(round_nums, losses, label='Loss', color='red', marker='x')\n",
    "    ax1.set_xlabel('Round')\n",
    "    ax1.set_ylabel('Loss', color='red')\n",
    "    ax1.tick_params(axis='y', labelcolor='red')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(round_nums, accuracies, label='Accuracy', color='blue', marker='o')\n",
    "    ax2.set_ylabel('Accuracy', color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{output_path}/probability_loss_accuracy.png\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def plot_combined_roc_threshold(fpr, tpr, thresholds, auc_roc, optimal_threshold, output_path):\n",
    "    \"\"\"\n",
    "    Genera una imagen con dos gráficos:\n",
    "    - Izquierda: Curva ROC.\n",
    "    - Derecha: TPR/FPR vs Threshold con umbral óptimo.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Gráfico 1: Curva ROC\n",
    "    axs[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {auc_roc:.2f})')\n",
    "    axs[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "    axs[0].set_xlabel('False Positive Rate')\n",
    "    axs[0].set_ylabel('True Positive Rate')\n",
    "    axs[0].set_title('ROC Curve')\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Gráfico 2: TPR/FPR vs Threshold\n",
    "    axs[1].plot(thresholds, tpr, label='True Positive Rate (TPR)', color='green', lw=2)\n",
    "    axs[1].plot(thresholds, fpr, label='False Positive Rate (FPR)', color='red', lw=2)\n",
    "    axs[1].axvline(optimal_threshold, color='blue', linestyle='--', label=f'Optimal Threshold ({optimal_threshold:.2f})')\n",
    "    axs[1].set_xlabel('Threshold')\n",
    "    axs[1].set_ylabel('Rate')\n",
    "    axs[1].set_title('TPR and FPR vs. Threshold')\n",
    "    axs[1].legend(loc=\"best\")\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{output_path}/roc_threshold.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_results_to_csv(results, transitions, metrics, custom_precision):\n",
    "    \"\"\"\n",
    "    Guarda los resultados en tres CSV:\n",
    "    - Resultados detallados por ronda.\n",
    "    - Transiciones entre rondas.\n",
    "    - Resultados finales (métricas globales).\n",
    "    \"\"\"\n",
    "    # Resultados por Ronda\n",
    "    with open(f\"{PREFIJO_SAVE}/round_results.csv\", 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Round', 'Prediction', 'Probability', 'Clients with Property', \n",
    "                      'Clients without Property', 'Has Property', 'Loss', 'Accuracy']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r in results:\n",
    "            writer.writerow({\n",
    "                'Round': r['round'],\n",
    "                'Prediction': r['prediction'],\n",
    "                'Probability': r['property_probability'],\n",
    "                'Clients with Property': r['clients_with_property'],\n",
    "                'Clients without Property': r['clients_without_property'],\n",
    "                'Has Property': r['has_property'],\n",
    "                'Loss': r['loss'],\n",
    "                'Accuracy': r['accuracy']\n",
    "            })\n",
    "\n",
    "    # Transiciones\n",
    "    with open(f\"{PREFIJO_SAVE}/transitions.csv\", 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Round', 'Prev Probability', 'Current Probability', 'Transition']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for t in transitions:\n",
    "            writer.writerow({\n",
    "                'Round': t['round'],\n",
    "                'Prev Probability': t['prev_probability'],\n",
    "                'Current Probability': t['current_probability'],\n",
    "                'Transition': t['transition']\n",
    "            })\n",
    "\n",
    "    # Métricas Finales\n",
    "    with open(f\"{PREFIJO_SAVE}/final_metrics.csv\", 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Metric', 'Value']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for key, value in metrics.items():\n",
    "            writer.writerow({'Metric': key, 'Value': value})\n",
    "        writer.writerow({'Metric': 'Custom Precision', 'Value': f\"{custom_precision:.2f}%\"})\n",
    "\n",
    "def calculate_and_log_metrics(results):\n",
    "    \"\"\"\n",
    "    Calcula las métricas principales y devuelve un diccionario con los resultados.\n",
    "    \"\"\"\n",
    "    y_true = [r['has_property'] for r in results][3:]  # Ignorar las 3 primeras rondas\n",
    "    y_prob = [r['property_probability'] for r in results][3:]\n",
    "    y_pred = [1 if prob > PROPERTY_THRESHOLD else 0 for prob in y_prob]\n",
    "\n",
    "    # Calcular ROC, precisión, recall, F1-score, y matriz de confusión\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Precisión basada en transiciones de probabilidad\n",
    "    correct_transitions, incorrect_transitions, dynamic_threshold = calculate_correct_predictions(results)\n",
    "    total_transitions = correct_transitions + incorrect_transitions\n",
    "    custom_precision = (correct_transitions / total_transitions) * 100 if total_transitions > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        'ROC AUC': auc_roc,\n",
    "        'Optimal Threshold': optimal_threshold,\n",
    "        'Dynamic Threshold': dynamic_threshold,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'True Positive': tp,\n",
    "        'False Positive': fp,\n",
    "        'True Negative': tn,\n",
    "        'False Negative': fn,\n",
    "    }\n",
    "\n",
    "    return metrics, fpr, tpr, thresholds, custom_precision\n",
    "\n",
    "\n",
    "def analyze_probability_transitions(results, prob_range=PROB_RANGE):\n",
    "    \"\"\"\n",
    "    Analiza las transiciones de probabilidad entre rondas, ignorando valores extremos y \n",
    "    ajustando las clasificaciones de transiciones en función de un rango dinámico.\n",
    "    \"\"\"\n",
    "    transitions = []\n",
    "    probabilities = [r['property_probability'] for r in results][3:]  # Ignorar las primeras 3 rondas\n",
    "    rounds = [r['round'] for r in results][3:]  # Ignorar las primeras 3 rondas\n",
    "\n",
    "    # Filtrar valores extremos basados en desviación estándar\n",
    "    prob_mean = np.mean(probabilities)\n",
    "    prob_std = np.std(probabilities)\n",
    "    lower_bound = prob_mean - 2 * prob_std\n",
    "    upper_bound = prob_mean + 2 * prob_std\n",
    "    filtered_probs = [p if lower_bound <= p <= upper_bound else prob_mean for p in probabilities]\n",
    "\n",
    "    min_prob, max_prob = min(filtered_probs), max(filtered_probs)\n",
    "    range_value = max_prob - min_prob if max_prob > min_prob else 1\n",
    "    significant_move = range_value * prob_range\n",
    "\n",
    "    for i in range(1, len(filtered_probs)):\n",
    "        prev_prob = filtered_probs[i - 1]\n",
    "        curr_prob = filtered_probs[i]\n",
    "        diff = curr_prob - prev_prob\n",
    "        transition = \"unsure\"\n",
    "\n",
    "        # Clasificar transiciones con rango dinámico\n",
    "        if abs(diff) < significant_move:\n",
    "            transition = \"stable\"\n",
    "        elif diff > significant_move:\n",
    "            transition = \"increase\"\n",
    "        elif diff < -significant_move:\n",
    "            transition = \"decrease\"\n",
    "\n",
    "        transitions.append({\n",
    "            'round': rounds[i],\n",
    "            'prev_probability': prev_prob,\n",
    "            'current_probability': curr_prob,\n",
    "            'transition': transition\n",
    "        })\n",
    "\n",
    "    logger.info(f\"Transitions analyzed: {len(transitions)} transitions processed.\")\n",
    "    return transitions\n",
    "\n"
   ],
   "id": "b2a49a38cf731cb3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:25:27.003423Z",
     "start_time": "2025-02-03T05:25:26.994412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### EXPERIMENT ####\n",
    "def simulated_federated_learning(clients, shadow_models, global_model, num_rounds=NUM_ROUNDS, threshold=PROPERTY_THRESHOLD):\n",
    "    logger.info(f\"Starting simulated federated learning with {num_rounds} rounds\")\n",
    "    results = []\n",
    "\n",
    "    clients_with_property = [client for client in clients if client.data['has_property']]\n",
    "    clients_without_property = [client for client in clients if not client.data['has_property']]\n",
    "\n",
    "    logger.info(f\"{len(clients_with_property)} clients have the property.\")\n",
    "    logger.info(f\"{len(clients_without_property)} clients do not have the property.\")\n",
    "\n",
    "    for round_num in tqdm.tqdm(range(1, num_rounds + 1), desc=\"Federated Learning Progress\"):\n",
    "        logger.info(f\"Starting round {round_num}\")\n",
    "\n",
    "        # Elegir aleatoriamente si usar clientes con o sin propiedad\n",
    "        use_property_clients = random.choice([True, False])\n",
    "        selected_clients = clients_with_property if use_property_clients else clients_without_property\n",
    "\n",
    "        # Obtener pesos actuales del modelo global\n",
    "        global_weights = global_model.get_weights()\n",
    "\n",
    "        client_weights = []\n",
    "        client_updates = []\n",
    "\n",
    "        logger.info(f\"Training {len(selected_clients)} selected clients...\")\n",
    "\n",
    "        # Recopilar actualizaciones de los clientes\n",
    "        for client in selected_clients:\n",
    "            w, _, update_info = client.fit(global_weights)\n",
    "            client_weights.append(w)\n",
    "            client_updates.append(np.concatenate([u.flatten() for u in update_info['updates']]))\n",
    "\n",
    "        # Promediar pesos y actualizaciones\n",
    "        averaged_weights = [np.mean(layer, axis=0) for layer in zip(*client_weights)]\n",
    "        global_model.set_weights(averaged_weights)\n",
    "        averaged_updates = average_client_updates(client_updates)\n",
    "        \n",
    "        logger.info(f\"Round {round_num}: Averaged updates shape: {len(averaged_updates)}, Expected input dimension: {global_model.input_shape[1]}\")\n",
    "\n",
    "        # Aplicar flipping a los clientes seleccionados después del entrenamiento\n",
    "        if LABEL_FLIPPING and not FLIPPING_ANTES:\n",
    "            logger.info(f\"Applying label flipping AFTER local training for round {round_num}\")\n",
    "            for client in selected_clients:\n",
    "                client.data['y_label'] = apply_label_flipping(\n",
    "                    client.data['y_label'], FLIP_TARGET, PROB_FLIP_0, PROB_FLIP_1\n",
    "                )\n",
    "\n",
    "        # Inferencia de los modelos sombra\n",
    "        if ENTRENO_ANTES:\n",
    "            property_prob = infer_property(shadow_models, averaged_updates)\n",
    "        else:\n",
    "            property_prob = dynamic_inference_with_custom_model(averaged_updates)\n",
    "\n",
    "        # Evaluar el modelo global en todos los clientes\n",
    "        total_loss, total_accuracy, total_samples = 0, 0, 0\n",
    "        for client in clients:\n",
    "            loss, num_samples, metrics = client.evaluate(global_model.get_weights())\n",
    "            total_loss += loss * num_samples\n",
    "            total_accuracy += metrics['accuracy'] * num_samples\n",
    "            total_samples += num_samples\n",
    "\n",
    "        # Calcular métricas promedio de la ronda\n",
    "        avg_loss = total_loss / total_samples\n",
    "        avg_accuracy = total_accuracy / total_samples\n",
    "\n",
    "        # Guardar resultados\n",
    "        results.append({\n",
    "            'round': round_num,\n",
    "            'property_probability': property_prob,\n",
    "            'has_property': use_property_clients,\n",
    "            'prediction': property_prob > threshold,\n",
    "            'clients_with_property': len(clients_with_property) if use_property_clients else 0,\n",
    "            'clients_without_property': len(clients_without_property) if not use_property_clients else 0,\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': avg_accuracy\n",
    "        })\n",
    "\n",
    "        logger.info(f\"Round {round_num} completed. Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}, Property Prob: {property_prob:.4f}\")\n",
    "\n",
    "    logger.info(\"Federated learning simulation completed.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Paso 0: Validar configuración de parámetros\n",
    "    validate_configuration()\n",
    "\n",
    "    # Paso 1: Cargar y preprocesar datos\n",
    "    X, y_label, y_slice = load_and_preprocess_data(DATA_FILE_PATH)\n",
    "\n",
    "    # Paso 2: Dividir datos para modelo global y modelos sombra\n",
    "    (X_global, y_label_global, y_slice_global), (X_shadow, y_label_shadow, y_slice_shadow), _ = split_data_for_models(\n",
    "        X, y_label, y_slice\n",
    "    )\n",
    "\n",
    "    # Paso 3: Crear datos para los clientes con opciones de ruido y flipping antes del entrenamiento\n",
    "    logger.info(\"Creating client data with configured security options\")\n",
    "    client_data = create_client_data(X_global, y_label_global, y_slice_global)\n",
    "\n",
    "    # Paso 4: Crear e inicializar modelo global\n",
    "    logger.info(\"Initializing global model\")\n",
    "    global_model = create_global_model(client_data[0]['X'].shape[1])\n",
    "    \n",
    "    # Inicializar clientes simulados\n",
    "    clients = initialize_clients(client_data, global_model, GLOBAL_MODEL_EPOCHS, BATCH_SIZE)\n",
    "\n",
    "    # Paso 5: Entrenar modelos sombra (si ENTRENO_ANTES=True)\n",
    "    if ENTRENO_ANTES:\n",
    "        logger.info(\"Training shadow models prior to federated learning\")\n",
    "        shadow_models = train_shadow_models(\n",
    "            X_shadow, y_label_shadow, y_slice_shadow, NUM_SHADOW_MODELS, global_model, GLOBAL_MODEL_EPOCHS, BATCH_SIZE\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Skipping shadow model training (ENTRENO_ANTES=False)\")\n",
    "        shadow_models = []  # No se entrenan previamente\n",
    "\n",
    "    # Paso 6: Simulación de federated learning\n",
    "    logger.info(\"Starting federated learning simulation\")\n",
    "    results = simulated_federated_learning(clients, shadow_models, global_model)\n",
    "\n",
    "    # Paso 7: Analizar transiciones de probabilidad\n",
    "    logger.info(\"Analyzing probability transitions\")\n",
    "    transitions = analyze_probability_transitions(results, PROB_RANGE)\n",
    "\n",
    "    # Paso 8: Calcular métricas\n",
    "    logger.info(\"Calculating evaluation metrics\")\n",
    "    metrics, fpr, tpr, thresholds, custom_precision = calculate_and_log_metrics(results)\n",
    "\n",
    "    # Paso 9: Guardar resultados y generar gráficos\n",
    "    logger.info(\"Saving results and generating visualizations\")\n",
    "    save_results_to_csv(results, transitions, metrics, custom_precision)\n",
    "    plot_combined_probability_loss_accuracy(results, f\"{PREFIJO_SAVE}/\")\n",
    "    plot_combined_roc_threshold(fpr, tpr, thresholds, metrics['ROC AUC'], metrics['Optimal Threshold'], f\"{PREFIJO_SAVE}/\")\n",
    "\n",
    "    logger.info(\"Federated learning simulation completed\")\n"
   ],
   "id": "e6861ed7a278fbbf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-03T05:25:27.004427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "bd3a3dbcc20718a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 06:25:27,005 - __main__ - INFO - Validating configuration...\n",
      "2025-02-03 06:25:27,007 - __main__ - INFO - Ni ruido ni label flipping están activados. El experimento no incluye perturbaciones en los datos.\n",
      "2025-02-03 06:25:27,008 - __main__ - INFO - Configuration validation completed successfully.\n",
      "2025-02-03 06:25:27,008 - __main__ - INFO - Loading and preprocessing data from label_bi_10.csv\n",
      "2025-02-03 06:25:28,002 - __main__ - INFO - Columns after loading: ['Src IP', 'Src Port', 'Dst Port', 'Protocol', 'Flow Duration', 'Total Fwd Packet', 'Fwd Packet Length Std', 'ACK Flag Count', 'Fwd Seg Size Min', 'label', 'Slice']\n",
      "2025-02-03 06:25:28,002 - __main__ - INFO - Dividiendo datos para el modelo global y los modelos sombra.\n",
      "2025-02-03 06:25:28,013 - __main__ - INFO - Datos divididos: 122192 para el modelo global, 1234 para los modelos sombra.\n",
      "2025-02-03 06:25:28,013 - __main__ - INFO - Creating client data with configured security options\n",
      "2025-02-03 06:25:28,014 - __main__ - INFO - Creating data for 10 clients with security configurations\n",
      "2025-02-03 06:25:28,042 - __main__ - INFO - Initializing global model\n",
      "2025-02-03 06:25:28,042 - __main__ - INFO - Creating global model with input shape 9\n",
      "2025-02-03 06:25:28,172 - __main__ - INFO - Initializing simulated clients in parallel\n",
      "2025-02-03 06:25:28,174 - __main__ - INFO - Initializing simulated client 0\n",
      "2025-02-03 06:25:28,176 - __main__ - INFO - Initializing simulated client 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset (123426, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 06:25:28,181 - __main__ - INFO - Initializing simulated client 2\n",
      "2025-02-03 06:25:28,183 - __main__ - INFO - Initializing simulated client 3\n",
      "2025-02-03 06:25:28,184 - __main__ - INFO - Initializing simulated client 4\n",
      "2025-02-03 06:25:28,187 - __main__ - INFO - Initializing simulated client 5\n",
      "2025-02-03 06:25:28,190 - __main__ - INFO - Initializing simulated client 6\n",
      "2025-02-03 06:25:28,191 - __main__ - INFO - Initializing simulated client 7\n",
      "2025-02-03 06:25:28,196 - __main__ - INFO - Initializing simulated client 8\n",
      "2025-02-03 06:25:28,198 - __main__ - INFO - Initializing simulated client 9\n",
      "2025-02-03 06:25:28,341 - __main__ - INFO - Training shadow models prior to federated learning\n",
      "2025-02-03 06:25:28,341 - __main__ - INFO - Training 5 shadow models prior to federated learning.\n",
      "2025-02-03 06:25:28,342 - __main__ - INFO - Processing shadow model 1/1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x00000266C08563E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 06:25:31,451 - tensorflow - WARNING - 5 out of the last 5 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x00000266C08563E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x00000266C4A29B20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 06:25:32,121 - tensorflow - WARNING - 6 out of the last 6 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x00000266C4A29B20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2025-02-03 06:25:34,506 - __main__ - INFO - Processing shadow model 11/1234\n",
      "2025-02-03 06:25:40,627 - __main__ - INFO - Processing shadow model 21/1234\n",
      "2025-02-03 06:25:47,132 - __main__ - INFO - Processing shadow model 31/1234\n",
      "2025-02-03 06:25:53,101 - __main__ - INFO - Processing shadow model 41/1234\n",
      "2025-02-03 06:25:58,878 - __main__ - INFO - Processing shadow model 51/1234\n",
      "2025-02-03 06:26:04,673 - __main__ - INFO - Processing shadow model 61/1234\n",
      "2025-02-03 06:26:10,665 - __main__ - INFO - Processing shadow model 71/1234\n",
      "2025-02-03 06:26:16,954 - __main__ - INFO - Processing shadow model 81/1234\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a3cb4ce81af618d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4a4c5999afbddcd9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
